---
title: "Analysis of data"
output: html_notebook
---

```{r}
# Setup and load packages

pacman::p_load(
  rstanarm, 
  tidybayes, 
  tidyverse, 
  future, 
  future.apply
)

# Detecting cores
n_cores <- parallel::detectCores()
cat(paste("Running on UCloud with", n_cores, "cores available.\n"))

# Define the metrics we want to test
metrics_list <- c('lexical_jaccard', 'pos_jaccard', 'lsm_score', 
                  'semantic_similarity', 'sentiment_similarity')

# Run all metrics at once With 32 cores, 5 metrics will run instantly at the same time.
plan(multisession, workers = n_cores)
```

```{r}
# Load data & clean
# Make sure the path is correct relative to your Rmd file
df <- read.csv("../../data/outputs/merged.csv")

# 1. Basic Cleaning
df_clean <- df %>%
  drop_na(all_of(metrics_list), combined_topic_id, direction) %>%
  mutate(abs_turn = abs(turn))

# 2. Filter A: Conversation Length (Outliers)
conv_stats <- df_clean %>%
  group_by(conv_id) %>%
  summarise(n_turns = n())

len_95 <- quantile(conv_stats$n_turns, 0.95)

valid_conv_ids <- conv_stats %>%
  filter(n_turns >= 2, n_turns <= len_95) %>%
  pull(conv_id)

df_filtered <- df_clean %>%
  filter(conv_id %in% valid_conv_ids)

# 3. Filter B: Turn Support
turn_stats <- df_filtered %>%
  group_by(abs_turn) %>%
  summarise(n_convs = n_distinct(conv_id))

valid_turns <- turn_stats %>%
  filter(n_convs >= 200) %>%
  pull(abs_turn)

# Apply the turn filter
df_filtered <- df_filtered %>%
  filter(abs_turn %in% valid_turns)

cat(paste("Data loaded and filtered. Final N rows:", nrow(df_filtered)))
```

```{r}
# Feature engineering - topic labels

topic_info <- df_filtered %>%
  group_by(combined_topic_id) %>%
  summarise(
    raw_keywords = first(combined_keywords),
    n_convs = n_distinct(conv_id)
  ) %>%
  mutate(
    # Take first 2 keywords split by ' / '
    clean_keywords = map_chr(str_split(raw_keywords, " / "), 
                             ~ paste(.x[1:2], collapse = ", ")),
    topic_label = paste0(clean_keywords, " (n=", n_convs, ")")
  )

# Join back to main dataframe to create df_final
df_final <- df_filtered %>%
  left_join(topic_info %>% select(combined_topic_id, topic_label), 
            by = "combined_topic_id") %>%
  mutate(topic_label = as.factor(topic_label))

# Create Z-Scored Dataframe (df_z) for comparison
# We do this AFTER df_final is created
df_z <- df_final %>%
  mutate(across(all_of(metrics_list), ~ as.numeric(scale(.))))
```

```{r rq1_bayesian_robust}
# Worker Function
run_fast_bayes <- function(metric, data) {
  
  cat(paste("Worker started for:", metric, "\n"))
  
  f <- as.formula(paste(metric, "~ 0 + topic_label + (1 | conv_id)"))
  
  # Fit Model
  # We use chains=2 and iter=1000 for speed
  model <- stan_lmer(
    f, 
    data = data, 
    chains = 2, 
    iter = 1000, 
    seed = 123, 
    cores = 1, # Keep this 1. The parallelism is at the metric level.
    refresh = 0
  )
  
  # Extract Data
  draws <- model %>%
    gather_draws(b_topic_label[topic]) %>%
    mutate(
      topic_clean = str_remove(topic, "topic_label"),
      metric = metric
    )
  
  return(draws)
}

# 5. EXECUTE (The Fast Part)
cat("Launching 5 models simultaneously...\n")

# This runs the loop in parallel across your 32 cores
results_list <- future_lapply(metrics_list, function(m) {
  run_fast_bayes(m, df_z)
})

# 6. Save & Combine
all_draws <- bind_rows(results_list)
write_csv(all_draws, "bayesian_results.csv")

cat("DONE! Data saved to bayesian_results.csv")
```


