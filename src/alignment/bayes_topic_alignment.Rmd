---
title: "Bayes Topic Alignment (Publishable Baseline)"
output:
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## 0) Packages

```{r}
pacman::p_load(
  brms,
  cmdstanr,
  tidyverse,
  readr,
  tidybayes,
  posterior,
  bayesplot,
  pheatmap,
  dendextend
)
```

## 1) Model specification and run configuration

```{r}
set.seed(123)

metrics_list <- c(
  "lexical_jaccard",
  "pos_jaccard",
  "lsm_score",
  "semantic_similarity",
  "sentiment_similarity"
)

# Topic-only model with conversation random intercept
my_priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("exponential(1)", class = "sd"),
  set_prior("exponential(1)", class = "sigma")
)

# Use cmdstanr if available, otherwise fall back to rstan
if (requireNamespace("cmdstanr", quietly = TRUE) &&
    !is.na(cmdstanr::cmdstan_version(error_on_NA = FALSE))) {
  backend_to_use <- "cmdstanr"
} else {
  backend_to_use <- "rstan"
}

options(mc.cores = parallel::detectCores())

# Publishable sampling settings
chains_pub <- 4
iter_pub   <- 4000
warmup_pub <- 1000
sampling_ctrl <- list(adapt_delta = 0.95, max_treedepth = 12)

# Compilation settings for cmdstanr
stan_opt <- list(stanc_options = list("O1"))
threads_per_chain <- 12
use_threading <- backend_to_use == "cmdstanr"
thread_args <- if (use_threading) {
  list(threads = threading(threads_per_chain))
} else {
  list()
}

cat("Backend:", backend_to_use, "\n")
if (backend_to_use == "cmdstanr") {
  cat("CmdStan version:", cmdstanr::cmdstan_version(), "\n")
}

brm_base_args <- list(
  family = gaussian(),
  prior = my_priors,
  chains = chains_pub,
  iter = iter_pub,
  warmup = warmup_pub,
  seed = 123,
  backend = backend_to_use,
  cores = chains_pub,
  refresh = 0,
  control = sampling_ctrl,
  silent = 2,
  file_refit = "on_change"
)
```

## 2) Load + clean + filter

```{r}
df <- read_csv("../../data/outputs/merged.csv", show_col_types = FALSE)

df_clean <- df %>%
  select(conv_id, combined_topic_id, combined_keywords, all_of(metrics_list)) %>%
  drop_na(all_of(metrics_list), combined_topic_id, conv_id, combined_keywords) %>%
  mutate(conv_id = as.factor(conv_id))

# Filter A: conversation length outliers
conv_stats <- df_clean %>%
  group_by(conv_id) %>%
  summarise(n_turns = n(), .groups = "drop")

len_95 <- quantile(conv_stats$n_turns, 0.95, na.rm = TRUE)
valid_conv_ids <- conv_stats %>%
  filter(n_turns >= 2, n_turns <= len_95) %>%
  pull(conv_id)

df_filtered <- df_clean %>%
  filter(conv_id %in% valid_conv_ids)

cat("Final N rows:", nrow(df_filtered), "\n")
cat("N conversations:", n_distinct(df_filtered$conv_id), "\n")
```

## 3) Topic labels and z-scoring

```{r}
topic_info <- df_filtered %>%
  group_by(combined_topic_id) %>%
  summarise(
    raw_keywords = first(combined_keywords),
    n_convs = n_distinct(conv_id),
    .groups = "drop"
  ) %>%
  mutate(
    clean_keywords = map_chr(str_split(raw_keywords, " / "),
                             ~ paste(.x[1:min(2, length(.x))], collapse = ", ")),
    topic_label = paste0(clean_keywords, " (n=", n_convs, ")"),
    topic_label = as.factor(topic_label)
  )

df_final <- df_filtered %>%
  left_join(topic_info %>% dplyr::select(combined_topic_id, topic_label),
            by = "combined_topic_id")

df_z <- df_final %>%
  mutate(across(all_of(metrics_list), ~ as.numeric(scale(.))))
```

## 4) Fit models (one per metric)

Outputs (under data/outputs/bayes):
- bayes_topics_draws.csv
- bayes_topics_hierarchical_stats.csv
- bayes_topics_diagnostics.csv
- bayes_topics_ppc_*.png
- diagnostics/ (trace, rank, neff, rhat, sd densities)
- figures/ (topic effect plots, rank probs, centered effects)

```{r}
draws_list <- list()
hierarchy_stats <- list()
diagnostics <- list()

output_dir <- "../../data/outputs/bayes/bayes_topic_alignment_outputs"
models_dir <- file.path(output_dir, "models")
fig_dir <- file.path(output_dir, "figures")
tables_dir <- file.path(output_dir, "tables")
diag_dir <- file.path(output_dir, "diagnostics")
core_fig_dir <- file.path(fig_dir, "core")
ppc_dir <- file.path(output_dir, "ppc")
resid_dir <- file.path(output_dir, "residuals")
dir.create(models_dir, showWarnings = FALSE, recursive = TRUE)
dir.create(fig_dir, showWarnings = FALSE, recursive = TRUE)
dir.create(tables_dir, showWarnings = FALSE, recursive = TRUE)
dir.create(diag_dir, showWarnings = FALSE, recursive = TRUE)
dir.create(core_fig_dir, showWarnings = FALSE, recursive = TRUE)
dir.create(ppc_dir, showWarnings = FALSE, recursive = TRUE)
dir.create(resid_dir, showWarnings = FALSE, recursive = TRUE)
```


```{r}
for (metric in metrics_list) {

  cat("\n===================================================\n")
  cat("STARTING:", metric, "Time:", format(Sys.time(), "%H:%M:%S"), "\n")

  f <- as.formula(paste(metric, "~ 0 + topic_label + (1 | conv_id)"))

  brm_args <- c(
    list(
      formula = f,
      data = df_z,
      file = file.path(models_dir, paste0("brm_", metric))
    ),
    brm_base_args
  )

  if (backend_to_use == "cmdstanr") {
    brm_args <- c(
      brm_args,
      list(
        stan_model_args = stan_opt,
        threads = thread_args$threads
      )
    )
  }

  fit <- do.call(brm, brm_args)

  saveRDS(fit, file = file.path(models_dir, paste0("brm_", metric, ".rds")))

  # Diagnostics: Rhat and ESS for fixed effects
  summ <- summary(fit)
  rhat_max <- max(summ$fixed[,"Rhat"], na.rm = TRUE)
  ess_min <- min(summ$fixed[,"Bulk_ESS"], na.rm = TRUE)

  diagnostics[[metric]] <- tibble(
    metric = metric,
    rhat_max_fixed = as.numeric(rhat_max),
    ess_min_fixed = as.numeric(ess_min)
  )

  # Hierarchical stats
  vc <- VarCorr(fit)
  conv_sd <- vc$conv_id$sd[1]
  residual_sd <- if (!is.null(vc$residual$sd[1])) vc$residual$sd[1] else NA

  hierarchy_stats[[metric]] <- tibble(
    metric = metric,
    conv_sd = as.numeric(conv_sd),
    residual_sd = as.numeric(residual_sd)
  )

  # Topic coefficient draws
  draws_raw <- as_draws_df(fit)
  topic_cols <- names(draws_raw)[str_detect(names(draws_raw), "^b_topic_label")]

  draws <- draws_raw %>%
    select(all_of(topic_cols), .chain, .iteration, .draw) %>%
    pivot_longer(
      cols = all_of(topic_cols),
      names_to = "full_param_name",
      values_to = "value"
    ) %>%
    mutate(
      topic_clean = str_remove(full_param_name, "^b_topic_label"),
      metric = metric
    )

  draws_list[[metric]] <- draws

  write_csv(bind_rows(draws_list), file.path(output_dir, "bayes_topics_draws.csv"))
  write_csv(bind_rows(hierarchy_stats), file.path(output_dir, "bayes_topics_hierarchical_stats.csv"))
  write_csv(bind_rows(diagnostics), file.path(output_dir, "bayes_topics_diagnostics.csv"))

  # Posterior predictive check
  ppc_plot <- pp_check(fit, ndraws = 100, type = "dens_overlay") +
    ggtitle(paste("Posterior predictive check -", metric))
  png(filename = file.path(ppc_dir, paste0("bayes_topics_ppc_", metric, ".png")), width = 1000, height = 700)
  print(ppc_plot)
  dev.off()

  rm(fit, draws_raw, draws, vc)
  gc()

  cat("DONE:", metric, "\n")
}
```

## 5) PPCs only (from saved models)

Run this section independently to regenerate posterior predictive checks
without refitting models.

```{r}
for (metric in metrics_list) {
  model_path <- file.path(models_dir, paste0("brm_", metric, ".rds"))
  if (!file.exists(model_path)) {
    warning("Missing model file for metric: ", metric)
    next
  }

  fit <- readRDS(model_path)
  ppc_plot <- pp_check(fit, ndraws = 100, type = "dens_overlay") +
    ggtitle(paste("Posterior predictive check -", metric))

  png(filename = file.path(ppc_dir, paste0("bayes_topics_ppc_", metric, ".png")),
      width = 1000, height = 700)
  print(ppc_plot)
  dev.off()

  rm(fit)
  gc()
}
```

## 6) Topic effects plot

```{r fig.height=12, fig.width=12}
topic_draws <- read_csv(file.path(output_dir, "bayes_topics_draws.csv"), show_col_types = FALSE)

topic_summ <- topic_draws %>%
  group_by(metric, topic_clean) %>%
  median_qi(value, .width = 0.89) %>%
  ungroup()

topic_summ <- topic_summ %>%
  group_by(metric) %>%
  mutate(topic_clean = fct_reorder(topic_clean, value)) %>%
  ungroup()

```

```{r fig.height=10, fig.width=12}
# Save the primary topic effects plot
p_topic_effects <- ggplot(topic_summ, aes(x = value, y = topic_clean)) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  ggdist::geom_pointinterval(
    aes(xmin = .lower, xmax = .upper),
    size = 0.3
  ) +
  facet_wrap(~metric, scales = "free_y") +
  labs(
    title = "Topic effects with 89 percent intervals",
    x = "Posterior effect on standardized alignment (z)",
    y = NULL
  ) +
  theme_bw() +
  theme(
    strip.text = element_text(face = "bold"),
    axis.text.y = element_text(size = 7)
  )

p_topic_effects
ggsave(file.path(fig_dir, "topic_effects_pointinterval.png"), p_topic_effects,
       width = 12, height = 10, dpi = 300)
```

## 6.1) Summary tables and core figures

```{r}
# Summary table: topic effects with probabilities and intervals
topic_prob_pos <- topic_draws %>%
  group_by(metric, topic_clean) %>%
  summarise(
    p_pos = mean(value > 0),
    p_neg = mean(value < 0),
    .groups = "drop"
  )

topic_rank_probs <- topic_draws %>%
  group_by(metric, .draw) %>%
  mutate(rank = rank(-value, ties.method = "average")) %>%
  ungroup() %>%
  group_by(metric, topic_clean) %>%
  summarise(
    p_top1 = mean(rank == 1),
    p_top3 = mean(rank <= 3),
    .groups = "drop"
  )

topic_sizes <- topic_info %>%
  select(topic_label, n_convs) %>%
  rename(topic_clean = topic_label)

topic_effects_summary <- topic_draws %>%
  group_by(metric, topic_clean) %>%
  median_qi(value, .width = 0.89) %>%
  ungroup() %>%
  rename(median = value, lower = .lower, upper = .upper) %>%
  left_join(topic_prob_pos, by = c("metric", "topic_clean")) %>%
  left_join(topic_rank_probs, by = c("metric", "topic_clean")) %>%
  left_join(topic_sizes, by = "topic_clean") %>%
  arrange(metric, desc(median))

write_csv(topic_effects_summary, file.path(tables_dir, "topic_effects_summary.csv"))

top_by_median <- topic_effects_summary %>%
  group_by(metric) %>%
  slice_max(median, n = 5) %>%
  ungroup()

top_by_p_pos <- topic_effects_summary %>%
  group_by(metric) %>%
  slice_max(p_pos, n = 5) %>%
  ungroup()

write_csv(top_by_median, file.path(tables_dir, "top_topics_by_median.csv"))
write_csv(top_by_p_pos, file.path(tables_dir, "top_topics_by_p_pos.csv"))

topic_effects_summary %>% head(10)
```

## 7) Diagnostic checklist

```{r}
pacman::p_load(brms, posterior,bayesplot,tidyverse)

models_dir <- file.path(output_dir, "models")
model_files <- list.files(models_dir, pattern = "\\.rds$", full.names = TRUE)

```

```{r}
diag_table <- map_dfr(model_files, function(f) {
  fit <- readRDS(f)
  summ <- summary(fit)
  
  draws <- as_draws_df(fit)
  
  tibble(
    metric = gsub("^brm_|\\.rds$", "", basename(f)),
    rhat_max_fixed = max(summ$fixed[, "Rhat"], na.rm = TRUE),
    ess_min_fixed  = min(summ$fixed[, "Bulk_ESS"], na.rm = TRUE),
    n_divergent    = sum(draws$divergent__ == 1, na.rm = TRUE)
  )
})

write_csv(diag_table, file.path(output_dir, "diagnostics_convergence.csv"))
diag_table
```
```{r}
topic_dispersion <- topic_summ %>%
  group_by(topic_clean) %>%
  summarise(
    dispersion = sd(value),
    .groups = "drop"
  )

ggplot(topic_dispersion, aes(x = reorder(topic_clean, dispersion), y = dispersion)) +
  geom_col() +
  coord_flip() +
  labs(
    y = "SD of topic effects across metrics",
    x = NULL,
    title = "Metric disagreement within topics"
  ) +
  theme_minimal()

```

```{r topic-clustered-heatmap, fig.width=18, fig.height=21, dpi=300}
# Clustered heatmap of topic effects across metrics (column-scaled)
topic_matrix <- topic_summ %>%
  select(topic_clean, metric, value) %>%
  pivot_wider(names_from = metric, values_from = value) %>%
  column_to_rownames("topic_clean") %>%
  as.matrix()

topic_matrix <- topic_matrix[complete.cases(topic_matrix), , drop = FALSE]
topic_matrix_scaled <- scale(topic_matrix)
topic_matrix_scaled <- t(topic_matrix_scaled)

heatmap_path <- file.path(fig_dir, "topic_effects_heatmap_clustered.png")
max_abs <- max(abs(topic_matrix_scaled), na.rm = TRUE)
heatmap_breaks <- seq(-max_abs, max_abs, length.out = 101)

if (nrow(topic_matrix_scaled) > 1 && ncol(topic_matrix_scaled) > 1) {
  dist_mat <- dist(t(topic_matrix_scaled), method = "euclidean")
  hc <- hclust(dist_mat, method = "ward.D2")
  col_clusters <- cutree(hc, k = 5)
  col_clusters <- factor(col_clusters, levels = sort(unique(col_clusters)))
  annotation_col <- data.frame(Metrics = col_clusters)
  rownames(annotation_col) <- colnames(topic_matrix_scaled)
  cluster_colors <- setNames(
    c("#1b9e77", "#d95f02", "#7570b3", "#e7298a", "#66a61e")[seq_along(levels(col_clusters))],
    levels(col_clusters)
  )
  annotation_colors <- list(Metrics = cluster_colors)
  row_labels <- stringr::str_wrap(rownames(topic_matrix_scaled), width = 18)
  p_heatmap <- pheatmap::pheatmap(
    topic_matrix_scaled,
    breaks = heatmap_breaks,
    color = colorRampPalette(c("#b2182b", "#f7f7f7", "#2166ac"))(100),
    clustering_method = "ward.D2",
    clustering_distance_rows = "euclidean",
    clustering_distance_cols = "euclidean",
    cluster_cols = hc,
    border_color = NA,
    fontsize_row = 10,
    fontsize_col = 10,
    cellwidth = 26,
    cellheight = 24,
    angle_col = 45,
    labels_row = row_labels,
    annotation_col = annotation_col,
    annotation_colors = annotation_colors,
    treeheight_row = 40,
    treeheight_col = 40,
    main = "Topic effects clustered across alignment metrics",
    silent = TRUE
  )
  if (!is.null(p_heatmap$tree_col)) {
    col_order <- p_heatmap$tree_col$order
    col_ids <- colnames(topic_matrix_scaled)[col_order]
    label_colors <- cluster_colors[as.character(col_clusters[col_ids])]
    recolor_text <- function(g, cols) {
      if (inherits(g, "text")) {
        g$gp$col <- cols
        return(g)
      }
      if (!is.null(g$children)) {
        g$children <- lapply(g$children, recolor_text, cols = cols)
      }
      if (!is.null(g$grobs)) {
        g$grobs <- lapply(g$grobs, recolor_text, cols = cols)
      }
      g
    }
    colnames_grob_idx <- which(grepl("col_names", p_heatmap$gtable$layout$name))
    if (length(colnames_grob_idx) == 1) {
      colnames_grob <- p_heatmap$gtable$grobs[[colnames_grob_idx]]
      colnames_grob <- recolor_text(colnames_grob, label_colors)
      p_heatmap$gtable$grobs[[colnames_grob_idx]] <- colnames_grob
    }
  }
  p_heatmap$gtable$widths[1] <- p_heatmap$gtable$widths[1] + grid::unit(1.5, "cm")

  png(heatmap_path, width = 18, height = 21, units = "in", res = 300)
  grid::grid.newpage()
  grid::grid.draw(p_heatmap$gtable)
  dev.off()

  grid::grid.newpage()
  grid::grid.draw(p_heatmap$gtable)
}
```



