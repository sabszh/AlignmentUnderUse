---
title: "Analysis of data"
---

```{r}
# Setup and load packages
pacman::p_load(
  brms, 
  tidyverse, 
  readr, 
  tidybayes, 
  viridis,
  gridExtra,
  ggridges,
  pheatmap
)

options(parallelly.availableCores.methods = "system")

# Define metrics
metrics_list <- c('lexical_jaccard', 'pos_jaccard', 'lsm_score', 
                  'semantic_similarity', 'sentiment_similarity')
```

```{r}

```


```{r}
# Load data & clean
# Make sure the path is correct relative to your Rmd file
df <- read_csv("../../data/outputs/merged.csv", show_col_types = FALSE)

# 1. Basic Cleaning
df_clean <- df %>%
  drop_na(all_of(metrics_list), combined_topic_id, direction) %>%
  mutate(abs_turn = abs(turn))

# 2. Filter A: Conversation Length (Outliers)
conv_stats <- df_clean %>%
  group_by(conv_id) %>%
  summarise(n_turns = n())

len_95 <- quantile(conv_stats$n_turns, 0.95)

valid_conv_ids <- conv_stats %>%
  filter(n_turns >= 2, n_turns <= len_95) %>%
  pull(conv_id)

df_filtered <- df_clean %>%
  filter(conv_id %in% valid_conv_ids)

# 3. Filter B: Turn Support
turn_stats <- df_filtered %>%
  group_by(abs_turn) %>%
  summarise(n_convs = n_distinct(conv_id))

valid_turns <- turn_stats %>%
  filter(n_convs >= 200) %>%
  pull(abs_turn)

# Apply the turn filter
df_filtered <- df_filtered %>%
  filter(abs_turn %in% valid_turns)

cat(paste("Data loaded and filtered. Final N rows:", nrow(df_filtered)))
```

```{r}
# Feature engineering - topic labels

topic_info <- df_filtered %>%
  group_by(combined_topic_id) %>%
  summarise(
    raw_keywords = first(combined_keywords),
    n_convs = n_distinct(conv_id)
  ) %>%
  mutate(
    # Take first 2 keywords split by ' / '
    clean_keywords = map_chr(str_split(raw_keywords, " / "), 
                             ~ paste(.x[1:2], collapse = ", ")),
    topic_label = paste0(clean_keywords, " (n=", n_convs, ")")
  )

# Join back to main dataframe to create df_final
df_final <- df_filtered %>%
  left_join(topic_info %>% select(combined_topic_id, topic_label), 
            by = "combined_topic_id") %>%
  mutate(topic_label = as.factor(topic_label))

# Create Z-Scored Dataframe (df_z) for comparison
# We do this AFTER df_final is created
df_z <- df_final %>%
  mutate(across(all_of(metrics_list), ~ as.numeric(scale(.))))
```

```{r}
# Setting priors
my_priors <- c(
  set_prior("normal(0, 1)", class = "b"),       
  set_prior("exponential(1)", class = "sd"),    
  set_prior("exponential(1)", class = "sigma")  
)
```

```{r}
# Run Loop
# 1. Setup Storage
draws_list <- list()      
hierarchy_stats <- list() 

# 2. Run Loop
for (metric in metrics_list) {
  
  cat(paste0("\n===================================================\n"))
  cat(paste0("ðŸš€ STARTING: ", metric, "  (Time: ", format(Sys.time(), "%H:%M:%S"), ")\n"))
  
  f <- as.formula(paste(metric, "~ 0 + topic_label + (1 | conv_id)"))
  
  # Fit Model
  fit <- brm(
    formula = f,
    data = df_z,
    prior = my_priors,    
    family = gaussian(),
    chains = 2,
    iter = 1000,
    warmup = 500,
    seed = 123,
    backend = "rstan",
    cores = 2,        
    refresh = 100     
  )
  
  # --- A. CAPTURE HIERARCHICAL STATS ---
  vc <- VarCorr(fit)
  conv_sd <- vc$conv_id$sd[1]
  
  # Safety check: if residual SD is missing (rare), set to NA
  residual_sd <- if (!is.null(vc$residual$sd[1])) vc$residual$sd[1] else NA
  
  hierarchy_stats[[metric]] <- data.frame(
    Metric = metric,
    Conv_SD = conv_sd,       
    Residual_SD = residual_sd 
  )
  
  cat(paste0("   ðŸ“Š RESULT: Conversation SD = ", round(conv_sd, 3), "\n"))
  
  # --- B. CAPTURE TOPIC FINGERPRINTS (THE NUCLEAR FIX) ---
  # Instead of gather_draws, we convert to a plain dataframe and pivot manually.
  # This bypasses all special character issues.
  
  draws_raw <- as_draws_df(fit)
  
  # 1. Identify the columns that start with "b_topic_label"
  # This works regardless of what special characters are in the name
  topic_cols <- names(draws_raw)[str_detect(names(draws_raw), "^b_topic_label")]
  
  # 2. Select and Pivot
  draws <- draws_raw %>%
    select(all_of(topic_cols), .chain, .iteration, .draw) %>%
    pivot_longer(
      cols = all_of(topic_cols),
      names_to = "full_param_name",
      values_to = ".value"
    ) %>%
    mutate(
      # Remove the prefix to get the clean label back
      topic_clean = str_remove(full_param_name, "^b_topic_label"),
      metric = metric
    )
  
  draws_list[[metric]] <- draws
  
  # Save progress immediately
  temp_draws <- bind_rows(draws_list)
  temp_stats <- bind_rows(hierarchy_stats)
  write_csv(temp_draws, "bayesian_results_final.csv")
  write_csv(temp_stats, "hierarchical_stats.csv")
  
  # Clean memory
  rm(fit, draws, draws_raw, vc)
  gc()
  
  cat(paste0("âœ… DONE with ", metric, "!\n"))
}

cat("\nðŸŽ‰ ALL FINISHED! Files saved.")
```


```{r fig.height=15, fig.width=12}
# ==============================================================================
# VISUALIZATION: The Payoff
# ==============================================================================

# 1. Load the Results
all_draws <- read_csv("bayesian_results_final.csv")
hierarchy <- read_csv("hierarchical_stats.csv")

# 2. PLOT A: The Topic Fingerprints (Ridge Plot)
# ---------------------------------------------------------
# Sort topics by "Average Alignment" across all metrics to order the y-axis
topic_order <- all_draws %>%
  group_by(topic_clean) %>%
  summarise(mean_val = mean(.value)) %>%
  arrange(desc(mean_val)) %>% 
  pull(topic_clean)

# Prepare data
plot_data <- all_draws %>%
  mutate(topic_clean = factor(topic_clean, levels = topic_order)) %>%
  # Rename metrics for publication-ready labels
  mutate(metric_label = recode(metric,
    "lexical_jaccard" = "Lexical\n(Word Overlap)",
    "pos_jaccard" = "Syntax\n(Grammar)",
    "lsm_score" = "Style\n(Function Words)",
    "semantic_similarity" = "Semantic\n(Meaning)",
    "sentiment_similarity" = "Sentiment\n(Tone)"
  ))

# Generate Ridge Plot
p1 <- ggplot(plot_data, aes(x = .value, y = topic_clean, fill = stat(x))) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  
  # The Hills
  geom_density_ridges_gradient(
    scale = 1.5,
    rel_min_height = 0.01,
    alpha = 0.6
  ) +
  
  facet_grid(~ metric_label) +
  scale_fill_viridis_c(name = "Z-Score", option = "C") +
  
  labs(
    title = "Topic Alignment Fingerprints",
    subtitle = "Posterior distributions of alignment (Right = Higher Alignment)",
    x = "Standardized Alignment (Z-Score)",
    y = NULL
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.text.y = element_text(size = 9, face = "bold"),
    strip.text = element_text(face = "bold", size = 9),
    panel.grid.minor = element_blank()
  )

print(p1)

# 3. PLOT B: The "Who is Talking" Effect (Hierarchy)
# ---------------------------------------------------------
p2 <- ggplot(hierarchy, aes(x = reorder(Metric, -Conv_SD), y = Conv_SD)) +
  geom_col(fill = "steelblue", alpha = 0.8, width = 0.6) +
  geom_text(aes(label = round(Conv_SD, 2)), vjust = -0.5, fontface = "bold") +
  scale_y_continuous(limits = c(0, max(hierarchy$Conv_SD) * 1.2)) +
  labs(
    title = "The 'Partner Effect' by Metric",
    subtitle = "Standard Deviation of Conversation Intercepts (Higher = The specific pair matters more)",
    y = "Conversation SD (Sigma)",
    x = NULL
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p2)
```

```{r}
# ==============================================================================
# CHECK BASELINES (Raw Data)
# ==============================================================================

# Calculate raw averages (before Z-scoring)
baselines <- df_final %>%
  pivot_longer(cols = all_of(metrics_list), names_to = "metric", values_to = "value") %>%
  group_by(metric) %>%
  summarise(
    Mean_Raw = mean(value, na.rm = TRUE),
    SD_Raw = sd(value, na.rm = TRUE),
    Median_Raw = median(value, na.rm = TRUE)
  ) %>%
  mutate(metric_label = recode(metric,
    "lexical_jaccard" = "Lexical (Words)",
    "pos_jaccard" = "Syntax (Grammar)",
    "lsm_score" = "Style (Function Words)",
    "semantic_similarity" = "Semantic (Meaning)",
    "sentiment_similarity" = "Sentiment (Tone)"
  ))

print(baselines)

# Optional: Quick visual of the raw baselines
ggplot(baselines, aes(x = reorder(metric_label, Mean_Raw), y = Mean_Raw)) +
  geom_col(fill = "gray30", width = 0.6) +
  geom_text(aes(label = round(Mean_Raw, 3)), vjust = -0.5, fontface = "bold") +
  labs(
    title = "Global Baseline Alignment",
    subtitle = "How much does ChatGPT align on average? (Raw Scores)",
    y = "Average Score (0-1)",
    x = NULL
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
# ==============================================================================
# MINIMALIST CLUSTERING: No side colors, just gaps
# ==============================================================================

# 1. Prepare Data Matrix
cluster_data <- read_csv("bayesian_results_final.csv") %>%
  group_by(topic_clean, metric) %>%
  summarise(mean_z = mean(.value), .groups = "drop") %>%
  pivot_wider(names_from = metric, values_from = mean_z)

cluster_matrix <- cluster_data %>%
  select(-topic_clean) %>%
  as.matrix()

rownames(cluster_matrix) <- cluster_data$topic_clean

colnames(cluster_matrix) <- recode(colnames(cluster_matrix),
    "lexical_jaccard" = "Lexical",
    "pos_jaccard" = "Syntax",
    "lsm_score" = "Style",
    "semantic_similarity" = "Semantic",
    "sentiment_similarity" = "Sentiment"
)

# 2. Calculate Clustering for 5 Groups
d_rows <- dist(cluster_matrix, method = "euclidean")
hc_rows <- hclust(d_rows, method = "ward.D2")
cluster_assignments <- cutree(hc_rows, k = 10)

# 3. Re-order Matrix to group clusters together
ordered_indices <- order(cluster_assignments)
ordered_matrix <- cluster_matrix[ordered_indices, ]
ordered_clusters <- cluster_assignments[ordered_indices]

# 4. Find the Gap positions (where Cluster ID changes)
gap_locations <- which(diff(ordered_clusters) != 0)

# 5. Plot (Clean Version)
pheatmap(ordered_matrix,
         scale = "none",
         cluster_rows = FALSE,   # We use our manual order
         cluster_cols = TRUE,    # Let metrics cluster at the top
         
         # The Color Scale for the data
         color = hcl.colors(50, "RdBu", rev = TRUE),
         
         main = "Topic Alignment Modes (K=10)",
         fontsize_row = 9,
         angle_col = 45,
         
         # --- CLEANING UP ---
         annotation_row = NA,    # Removed the side colors!
         annotation_legend = FALSE,
         gaps_row = gap_locations # Keeps the physical space between text
)
```
```{r}
library(tidybayes); library(dplyr); library(ggplot2)

topic_summ <- all_draws %>%
  group_by(metric, topic_clean) %>%
  median_qi(.value, .width = .89)

ggplot(topic_summ, aes(x = .value, y = reorder(topic_clean, .value))) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_pointinterval() +
  facet_wrap(~metric, scales="free_x") +
  labs(x="Posterior (z)", y=NULL, title="Topic effects with 89% intervals")
```

```{r}

```

