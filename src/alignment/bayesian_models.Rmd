---
title: "Analysis of data"
---

```{r}
# Load packages used across the analysis and plotting blocks
pacman::p_load(
  brms, 
  tidyverse, 
  readr, 
  tidybayes, 
  viridis,
  ggridges,
  pheatmap
)

# Define the alignment metrics used throughout the notebook
metrics_list <- c('lexical_jaccard', 'pos_jaccard', 'lsm_score', 
                  'semantic_similarity', 'sentiment_similarity')
```

```{r}
# Load data and apply basic filtering used for all models
# Ensure the path is correct relative to this Rmd file
df <- read_csv("../../data/outputs/merged.csv", show_col_types = FALSE)

# 1) Basic cleaning: keep required fields and derive abs turn index
df_clean <- df %>%
  drop_na(all_of(metrics_list), combined_topic_id, direction) %>%
  mutate(abs_turn = abs(turn))

# 2) Filter A: remove extreme conversation lengths
conv_stats <- df_clean %>%
  group_by(conv_id) %>%
  summarise(n_turns = n())

len_95 <- quantile(conv_stats$n_turns, 0.95)

valid_conv_ids <- conv_stats %>%
  filter(n_turns >= 2, n_turns <= len_95) %>%
  pull(conv_id)

df_filtered <- df_clean %>%
  filter(conv_id %in% valid_conv_ids)

# 3) Filter B: remove turn positions with low support
turn_stats <- df_filtered %>%
  group_by(abs_turn) %>%
  summarise(n_convs = n_distinct(conv_id))

valid_turns <- turn_stats %>%
  filter(n_convs >= 200) %>%
  pull(abs_turn)

# Apply the turn filter
df_filtered <- df_filtered %>%
  filter(abs_turn %in% valid_turns)

cat(paste("Data loaded and filtered. Final N rows:", nrow(df_filtered)))
```

```{r}
# Feature engineering for readable topic labels

topic_info <- df_filtered %>%
  group_by(combined_topic_id) %>%
  summarise(
    raw_keywords = first(combined_keywords),
    n_convs = n_distinct(conv_id)
  ) %>%
  mutate(
    # Use the first two keywords as a short label
    clean_keywords = map_chr(str_split(raw_keywords, " / "), 
                             ~ paste(.x[1:2], collapse = ", ")),
    topic_label = paste0(clean_keywords, " (n=", n_convs, ")")
  )

# Join topic labels back to the main data set
df_final <- df_filtered %>%
  left_join(topic_info %>% dplyr::select(combined_topic_id, topic_label), 
            by = "combined_topic_id") %>%
  mutate(topic_label = as.factor(topic_label))

# Create z-scored versions of each metric for comparability
df_z <- df_final %>%
  mutate(across(all_of(metrics_list), ~ as.numeric(scale(.))))
```

```{r}
# Priors for the per-topic fixed effects and random intercepts
my_priors <- c(
  set_prior("normal(0, 1)", class = "b"),       
  set_prior("exponential(1)", class = "sd"),    
  set_prior("exponential(1)", class = "sigma")  
)
```

```{r}
# Fit one model per metric and store results to disk
# 1) Setup storage
draws_list <- list()      
hierarchy_stats <- list() 

# 2) Run loop
for (metric in metrics_list) {
  
  cat(paste0("\n===================================================\n"))
  cat(paste0("STARTING: ", metric, "  (Time: ", format(Sys.time(), "%H:%M:%S"), ")\n"))
  
  # Topic-only model with a conversation random intercept
  f <- as.formula(paste(metric, "~ 0 + topic_label + (1 | conv_id)"))
  
  # Fit the model
  fit <- brm(
    formula = f,
    data = df_z,
    prior = my_priors,    
    family = gaussian(),
    chains = 2,
    iter = 1000,
    warmup = 500,
    seed = 123,
    backend = "rstan",
    cores = 2,        
    refresh = 100     
  )
  
  # A) Capture random intercept and residual scale
  vc <- VarCorr(fit)
  conv_sd <- vc$conv_id$sd[1]
  
  # If residual SD is missing, keep NA
  residual_sd <- if (!is.null(vc$residual$sd[1])) vc$residual$sd[1] else NA
  
  hierarchy_stats[[metric]] <- data.frame(
    Metric = metric,
    Conv_SD = conv_sd,       
    Residual_SD = residual_sd 
  )
  
  cat(paste0("RESULT: Conversation SD = ", round(conv_sd, 3), "\n"))
  
  # B) Capture topic posteriors using a wide-to-long pivot
  # This avoids parsing problems in parameter names
  
  draws_raw <- as_draws_df(fit)
  
  # 1) Identify topic coefficient columns
  topic_cols <- names(draws_raw)[str_detect(names(draws_raw), "^b_topic_label")]
  
  # 2) Select and pivot to a tidy format
  draws <- draws_raw %>%
    select(all_of(topic_cols), .chain, .iteration, .draw) %>%
    pivot_longer(
      cols = all_of(topic_cols),
      names_to = "full_param_name",
      values_to = ".value"
    ) %>%
    mutate(
      # Remove the prefix to recover the topic label
      topic_clean = str_remove(full_param_name, "^b_topic_label"),
      metric = metric
    )
  
  draws_list[[metric]] <- draws
  
  # Save progress after each model
  temp_draws <- bind_rows(draws_list)
  temp_stats <- bind_rows(hierarchy_stats)
  write_csv(temp_draws, "bayesian_results_final.csv")
  write_csv(temp_stats, "hierarchical_stats.csv")
  
  # Free memory before the next model
  rm(fit, draws, draws_raw, vc)
  gc()
  
  cat(paste0("DONE with ", metric, "!\n"))
}

cat("\nALL FINISHED. Files saved.")
```


```{r fig.height=15, fig.width=12}
# ==============================================================================
# VISUALIZATION: The Payoff
# ==============================================================================

# 1) Load the saved posterior draws and hierarchy stats
all_draws <- read_csv("bayesian_results_final.csv")
hierarchy <- read_csv("hierarchical_stats.csv")

# 2) Plot A: topic fingerprints as posterior densities
# Sort topics by average alignment to order the y-axis
topic_order <- all_draws %>%
  group_by(topic_clean) %>%
  summarise(mean_val = mean(.value)) %>%
  arrange(desc(mean_val)) %>% 
  pull(topic_clean)

# Prepare labels for plotting
plot_data <- all_draws %>%
  mutate(topic_clean = factor(topic_clean, levels = topic_order)) %>%
  # Rename metrics for clearer facet labels
  mutate(metric_label = recode(metric,
    "lexical_jaccard" = "Lexical\n(Word Overlap)",
    "pos_jaccard" = "Syntax\n(Grammar)",
    "lsm_score" = "Style\n(Function Words)",
    "semantic_similarity" = "Semantic\n(Meaning)",
    "sentiment_similarity" = "Sentiment\n(Tone)"
  ))

# Generate ridge plot for topic posteriors
p1 <- ggplot(plot_data, aes(x = .value, y = topic_clean, fill = stat(x))) +
  geom_vline(xintercept = 0, linetype = "dashed", color = "gray50") +
  
  # The Hills
  geom_density_ridges_gradient(
    scale = 1.5,
    rel_min_height = 0.01,
    alpha = 0.6
  ) +
  
  facet_grid(~ metric_label) +
  scale_fill_viridis_c(name = "Z-Score", option = "C") +
  
  labs(
    title = "Topic Alignment Fingerprints",
    subtitle = "Posterior distributions of alignment (Right = Higher Alignment)",
    x = "Standardized Alignment (Z-Score)",
    y = NULL
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    axis.text.y = element_text(size = 9, face = "bold"),
    strip.text = element_text(face = "bold", size = 9),
    panel.grid.minor = element_blank()
  )

print(p1)

# 3) Plot B: conversation-level variance by metric
p2 <- ggplot(hierarchy, aes(x = reorder(Metric, -Conv_SD), y = Conv_SD)) +
  geom_col(fill = "steelblue", alpha = 0.8, width = 0.6) +
  geom_text(aes(label = round(Conv_SD, 2)), vjust = -0.5, fontface = "bold") +
  scale_y_continuous(limits = c(0, max(hierarchy$Conv_SD) * 1.2)) +
  labs(
    title = "The 'Partner Effect' by Metric",
    subtitle = "Standard Deviation of Conversation Intercepts (Higher = The specific pair matters more)",
    y = "Conversation SD (Sigma)",
    x = NULL
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(p2)
```

```{r}
# ==============================================================================
# CHECK BASELINES (Raw Data)
# ==============================================================================

# Calculate raw averages before z-scoring
baselines <- df_final %>%
  pivot_longer(cols = all_of(metrics_list), names_to = "metric", values_to = "value") %>%
  group_by(metric) %>%
  summarise(
    Mean_Raw = mean(value, na.rm = TRUE),
    SD_Raw = sd(value, na.rm = TRUE),
    Median_Raw = median(value, na.rm = TRUE)
  ) %>%
  mutate(metric_label = recode(metric,
    "lexical_jaccard" = "Lexical (Words)",
    "pos_jaccard" = "Syntax (Grammar)",
    "lsm_score" = "Style (Function Words)",
    "semantic_similarity" = "Semantic (Meaning)",
    "sentiment_similarity" = "Sentiment (Tone)"
  ))

print(baselines)

# Visualize the raw metric baselines
ggplot(baselines, aes(x = reorder(metric_label, Mean_Raw), y = Mean_Raw)) +
  geom_col(fill = "gray30", width = 0.6) +
  geom_text(aes(label = round(Mean_Raw, 3)), vjust = -0.5, fontface = "bold") +
  labs(
    title = "Global Baseline Alignment",
    subtitle = "How much does ChatGPT align on average? (Raw Scores)",
    y = "Average Score (0-1)",
    x = NULL
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
library(tidyverse)
library(igraph)

# Sensitivity grid for posterior overlap thresholds
eps_grid <- c(0.15, 0.20, 0.25)
tau_grid <- c(0.80, 0.90, 0.95)

# Run grid search for overlap-based clustering
results <- expand_grid(epsilon = eps_grid, tau = tau_grid)

run_overlap <- function(epsilon, tau) {

  prob_close <- pairs %>%
    group_by(topic.x, topic.y, metric) %>%
    summarise(prob = mean(diff < epsilon), .groups="drop")

  edges <- prob_close %>%
    group_by(topic.x, topic.y) %>%
    summarise(equiv = all(prob >= tau), .groups="drop") %>%
    filter(equiv)

  if (nrow(edges) == 0) {
    return(tibble(
      n_clusters = 0,
      largest_cluster = 0,
      n_singletons = NA
    ))
  }

  g <- graph_from_data_frame(edges, directed = FALSE)
  comp <- components(g)$membership

  sizes <- table(comp)

  tibble(
    n_clusters = length(sizes),
    largest_cluster = max(sizes),
    n_singletons = sum(sizes == 1)
  )
}

summary <- results %>%
  mutate(out = map2(epsilon, tau, run_overlap)) %>%
  unnest(out)
```


```{r}
library(tidyverse)
library(igraph)

# Load posterior draws in a compact format
post <- read_csv("bayesian_results_final.csv") %>%
  select(draw = .draw,
         topic = topic_clean,
         metric,
         mu = .value)

# Set thresholds for posterior overlap
epsilon <- 0.25
tau <- 0.90

# Build pairwise distances within each draw and metric
pairs <- post %>%
  inner_join(post, by = c("draw", "metric"),
             suffix = c(".x", ".y")) %>%
  filter(topic.x < topic.y) %>%
  mutate(diff = abs(mu.x - mu.y))

# Compute posterior probability that topic pairs are close
prob_close <- pairs %>%
  group_by(topic.x, topic.y, metric) %>%
  summarise(prob_close = mean(diff < epsilon),
            .groups = "drop")

# Keep edges where all metrics meet the threshold
equiv_edges <- prob_close %>%
  group_by(topic.x, topic.y) %>%
  summarise(all_metrics_equiv = all(prob_close >= tau),
            .groups = "drop") %>%
  filter(all_metrics_equiv)

# Build an undirected graph and compute components
g <- graph_from_data_frame(
  equiv_edges %>% select(topic.x, topic.y),
  directed = FALSE
)

cluster_df <- tibble(
  topic = V(g)$name,
  cluster = components(g)$membership
)

cluster_df %>% count(cluster, sort = TRUE)

```
```{r}
library(tidyverse)
library(pheatmap)

# Average posterior by topic and metric for a heatmap view
cluster_data <- read_csv("bayesian_results_final.csv") %>%
  group_by(topic_clean, metric) %>%
  summarise(mean_z = mean(.value), .groups = "drop") %>%
  pivot_wider(names_from = metric, values_from = mean_z)

cluster_matrix <- cluster_data %>%
  select(-topic_clean) %>%
  as.matrix()

rownames(cluster_matrix) <- cluster_data$topic_clean

colnames(cluster_matrix) <- recode(colnames(cluster_matrix),
  "lexical_jaccard" = "Lexical",
  "pos_jaccard" = "Syntax",
  "lsm_score" = "Style",
  "semantic_similarity" = "Semantic",
  "sentiment_similarity" = "Sentiment"
)

```
```{r}
cluster_lookup <- cluster_df %>%
  arrange(cluster, topic)

# Order topics by the posterior clustering
ordered_topics <- cluster_lookup$topic
ordered_clusters <- cluster_lookup$cluster

ordered_matrix <- cluster_matrix[ordered_topics, ]

```

```{r}
# Positions where cluster IDs change to create visual gaps
gap_locations <- which(diff(ordered_clusters) != 0)
```

```{r}
# Heatmap ordered by posterior clusters
pheatmap(
  ordered_matrix,
  scale = "none",
  cluster_rows = FALSE,   # order comes from posterior clusters
  cluster_cols = TRUE,

  color = hcl.colors(50, "RdBu", rev = TRUE),

  main = "Emergent Alignment Regimes (Posterior-Overlap Clustering)",
  fontsize_row = 9,
  angle_col = 45,

  annotation_row = NA,
  annotation_legend = FALSE,
  gaps_row = gap_locations
)

```



```{r}
# ==============================================================================
# MINIMALIST CLUSTERING: No side colors, just gaps
# ==============================================================================

# 1) Prepare the data matrix
cluster_data <- read_csv("bayesian_results_final.csv") %>%
  group_by(topic_clean, metric) %>%
  summarise(mean_z = mean(.value), .groups = "drop") %>%
  pivot_wider(names_from = metric, values_from = mean_z)

cluster_matrix <- cluster_data %>%
  select(-topic_clean) %>%
  as.matrix()

rownames(cluster_matrix) <- cluster_data$topic_clean

colnames(cluster_matrix) <- recode(colnames(cluster_matrix),
    "lexical_jaccard" = "Lexical",
    "pos_jaccard" = "Syntax",
    "lsm_score" = "Style",
    "semantic_similarity" = "Semantic",
    "sentiment_similarity" = "Sentiment"
)

# 2) Cluster topics based on mean alignment profiles
d_rows <- dist(cluster_matrix, method = "euclidean")
hc_rows <- hclust(d_rows, method = "ward.D2")
cluster_assignments <- cutree(hc_rows, k = 4)

# 3) Reorder the matrix to group topics by cluster
ordered_indices <- order(cluster_assignments)
ordered_matrix <- cluster_matrix[ordered_indices, ]
ordered_clusters <- cluster_assignments[ordered_indices]

# 4) Identify positions where the cluster label changes
gap_locations <- which(diff(ordered_clusters) != 0)

# 5) Plot the reordered heatmap
pheatmap(ordered_matrix,
         scale = "none",
         cluster_rows = FALSE,   # We use our manual order
         cluster_cols = TRUE,    # Let metrics cluster at the top
         
         # The Color Scale for the data
         color = hcl.colors(50, "RdBu", rev = TRUE),
         
         main = "Topic Alignment Modes (K=4)",
         fontsize_row = 9,
         angle_col = 45,
         
         # --- CLEANING UP ---
         annotation_row = NA,    # Removed the side colors!
         annotation_legend = FALSE,
         gaps_row = gap_locations # Keeps the physical space between text
)
```
```{r}
library(tidybayes); library(dplyr); library(ggplot2)

# Summarize topic posteriors with median and 89 percent interval
topic_summ <- all_draws %>%
  group_by(metric, topic_clean) %>%
  median_qi(.value, .width = .89)

ggplot(topic_summ, aes(x = .value, y = reorder(topic_clean, .value))) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  geom_pointinterval() +
  facet_wrap(~metric, scales="free_x") +
  labs(x="Posterior (z)", y=NULL, title="Topic effects with 89% intervals")
```

```{r}
library(tidybayes)
library(ggplot2)

# Aggregate posteriors within each cluster
clustered_post <- post %>%
  left_join(cluster_df, by = c("topic" = "topic"))

cluster_summaries <- clustered_post %>%
  group_by(cluster, metric, draw) %>%
  summarise(mu = mean(mu), .groups = "drop")

ggplot(cluster_summaries,
       aes(x = mu, y = factor(cluster), fill = metric)) +
  stat_halfeye(alpha = 0.7, slab_color = NA) +
  facet_wrap(~ metric, scales = "free_x") +
  geom_vline(xintercept = 0, linetype = "dashed", alpha = 0.4) +
  labs(
    x = "Posterior Alignment (Z)",
    y = "Alignment Regime",
    title = "Posterior Fingerprints of Emergent Alignment Regimes"
  ) +
  theme_minimal()

```
```{r}
# Build a posterior similarity matrix for topic pairs
posterior_distance <- prob_close %>%
  group_by(topic.x, topic.y) %>%
  summarise(
    mean_prob = mean(prob_close),
    .groups = "drop"
  )

dist_matrix <- posterior_distance %>%
  pivot_wider(names_from = topic.y, values_from = mean_prob)

pheatmap::pheatmap(
  as.matrix(dist_matrix[,-1]),
  cluster_rows = TRUE,
  cluster_cols = TRUE,
  color = hcl.colors(50, "RdBu", rev = TRUE),
  main = "Posterior Indistinguishability Between Topics"
)

```
```{r}
# Topics in the exact order used in the K=4 heatmap
topic_order <- rownames(ordered_matrix)

posterior_map <- cluster_df %>%
  rename(posterior_cluster = cluster) %>%
  filter(topic %in% topic_order) %>%
  mutate(row = match(topic, topic_order)) %>%
  arrange(row)
```

```{r}
# Summarize contiguous topic ranges by cluster
posterior_blocks <- posterior_map %>%
  group_by(posterior_cluster) %>%
  summarise(
    start = min(row),
    end = max(row),
    .groups = "drop"
  )

```

```{r}
library(pheatmap)

# Redraw the ordered heatmap without annotations
pheatmap(
  ordered_matrix,
  cluster_rows = FALSE,
  cluster_cols = TRUE,
  scale = "none",
  color = hcl.colors(50, "RdBu", rev = TRUE),
  fontsize_row = 9,
  angle_col = 45,
  main = "Topic Alignment Modes (K = 4)"
)

```
```{r}
# Load required libraries for consensus clustering
library(tidyverse)
library(pheatmap)

# 1) Load the posterior draws
# Ensure the file path is correct for your environment
df <- read.csv("bayesian_results_final.csv")

# 2) Setup parameters for consensus clustering
n_clusters <- 10
topics <- sort(unique(df$topic_clean))
n_topics <- length(topics)
# Use a subset of draws to balance reliability and runtime
set.seed(42)
sample_draws <- sample(unique(df$.draw), 100)

# 3) Initialize the agreement matrix
agreement_mat <- matrix(0, nrow = n_topics, ncol = n_topics)
rownames(agreement_mat) <- topics
colnames(agreement_mat) <- topics

print("Starting consensus clustering across draws...")

# 4) Loop through posterior draws and cluster each draw
for(d in sample_draws) {
  # Extract data for this draw and pivot to wide format
  draw_data <- df %>%
    filter(.draw == d) %>%
    select(topic_clean, metric, .value) %>%
    pivot_wider(names_from = metric, values_from = .value) %>%
    column_to_rownames("topic_clean")
  
  # Keep topics in a fixed order for consistent indexing
  draw_data <- draw_data[topics, ]
  
  # Perform clustering for this draw
  d_dist <- dist(draw_data)
  hc <- hclust(d_dist, method = "ward.D2")
  clusters <- cutree(hc, k = n_clusters)
  
  # Update agreement matrix based on co-clustering
  for(i in 1:n_topics) {
    for(j in 1:n_topics) {
      if(clusters[i] == clusters[j]) {
        agreement_mat[i, j] <- agreement_mat[i, j] + 1
      }
    }
  }
}

# 5) Normalize to a 0 to 1 scale
agreement_mat <- agreement_mat / length(sample_draws)

# 6) Visualize clustering stability
pheatmap(agreement_mat,
         clustering_method = "ward.D2",
         color = hcl.colors(50, "YlGnBu", rev = TRUE),
         main = "Consensus Matrix (Stability of K=10 Across Uncertainty)",
         display_numbers = FALSE,
         fontsize_row = 8,
         fontsize_col = 8)

# 7) Calculate stability scores for each topic
mean_profiles <- df %>%
  group_by(topic_clean, metric) %>%
  summarise(mean_val = mean(.value), .groups = "drop") %>%
  pivot_wider(names_from = metric, values_from = mean_val) %>%
  column_to_rownames("topic_clean")

mean_clusters <- cutree(hclust(dist(mean_profiles), method = "ward.D2"), k = n_clusters)

stability_results <- data.frame(
  topic = topics,
  cluster_id = mean_clusters[topics],
  stability_score = sapply(1:n_topics, function(i) {
    # Average probability of being clustered with members of its mean cluster
    peer_indices <- which(mean_clusters == mean_clusters[i])
    mean(agreement_mat[i, peer_indices])
  })
) %>% arrange(cluster_id, desc(stability_score))

# Save the stability report
print(stability_results)
write.csv(stability_results, "clustering_stability_report.csv", row.names = FALSE)
```




