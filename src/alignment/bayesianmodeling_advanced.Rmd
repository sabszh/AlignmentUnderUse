---
title: "Bayesian Alignment Analysis (Publishable Run)"
output:
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

## 0) Packages

```{r}
pacman::p_load(
  brms,
  cmdstanr,
  tidyverse,
  readr,
  tidybayes,
  posterior,
  ggdist,
  ggridges,
  pheatmap,
  bayesplot
)
```

## 1) Run configuration

```{r}
set.seed(123)

metrics_list <- c(
  "lexical_jaccard",
  "pos_jaccard",
  "lsm_score",
  "semantic_similarity",
  "sentiment_similarity"
)

# Use cmdstanr if available, otherwise fall back to rstan
if (requireNamespace("cmdstanr", quietly = TRUE) &&
    !is.na(cmdstanr::cmdstan_version(error_on_NA = FALSE))) {
  backend_to_use <- "cmdstanr"
} else {
  backend_to_use <- "rstan"
}

options(mc.cores = parallel::detectCores())
```

## 2) Load + clean + filter

```{r}
# Adjust path relative to this Rmd
df <- read_csv("../../data/outputs/merged.csv", show_col_types = FALSE)

# Keep only rows with required fields
df_clean <- df %>%
  drop_na(all_of(metrics_list), combined_topic_id, direction, turn, conv_id, combined_keywords) %>%
  mutate(
    abs_turn = abs(turn),
    direction = as.factor(direction),
    conv_id = as.factor(conv_id)
  )

# Filter A: conversation length outliers
conv_stats <- df_clean %>%
  group_by(conv_id) %>%
  summarise(n_turns = n(), .groups = "drop")

len_95 <- quantile(conv_stats$n_turns, 0.95, na.rm = TRUE)

valid_conv_ids <- conv_stats %>%
  filter(n_turns >= 2, n_turns <= len_95) %>%
  pull(conv_id)

df_filtered <- df_clean %>%
  filter(conv_id %in% valid_conv_ids)

# Filter B: turn support by absolute turn index
turn_stats <- df_filtered %>%
  group_by(abs_turn) %>%
  summarise(n_convs = n_distinct(conv_id), .groups = "drop")

valid_turns <- turn_stats %>%
  filter(n_convs >= 200) %>%
  pull(abs_turn)

df_filtered <- df_filtered %>%
  filter(abs_turn %in% valid_turns)

cat("Final N rows:", nrow(df_filtered), "\n")
cat("N conversations:", n_distinct(df_filtered$conv_id), "\n")
cat("Turn range:", min(df_filtered$abs_turn), "to", max(df_filtered$abs_turn), "\n")
```

## 3) Topic labels

```{r}
topic_info <- df_filtered %>%
  group_by(combined_topic_id) %>%
  summarise(
    raw_keywords = first(combined_keywords),
    n_convs = n_distinct(conv_id),
    .groups = "drop"
  ) %>%
  mutate(
    clean_keywords = map_chr(str_split(raw_keywords, " / "),
                             ~ paste(.x[1:min(2, length(.x))], collapse = ", ")),
    topic_label = paste0(clean_keywords, " (n=", n_convs, ")"),
    topic_label = as.factor(topic_label)
  )

df_final <- df_filtered %>%
  left_join(topic_info %>% dplyr::select(combined_topic_id, topic_label),
            by = "combined_topic_id")
```

## 4) Z-scoring

```{r}
df_long <- df_final %>%
  pivot_longer(all_of(metrics_list), names_to = "metric", values_to = "value")

df_long <- df_long %>%
  group_by(metric) %>%
  mutate(z = as.numeric(scale(value))) %>%
  ungroup()
```

## 5) Model specification (direction + turn dynamics)

```{r}
my_priors <- c(
  set_prior("normal(0, 1)", class = "b"),
  set_prior("exponential(1)", class = "sd"),
  set_prior("exponential(1)", class = "sigma")
)

sampling_ctrl <- list(adapt_delta = 0.95, max_treedepth = 12)

chains_pub <- 4
iter_pub   <- 4000
warmup_pub <- 1000
```

## 6) Benchmark (one metric)

This block runs a single metric with reduced iterations to estimate runtime.
Adjust `benchmark_metric` if you want a different representative metric.

```{r}
benchmark_metric <- "semantic_similarity"
benchmark_iter <- 800
benchmark_warmup <- 400

benchmark_data <- df_long %>% filter(metric == benchmark_metric)
benchmark_formula <- bf(z ~ 0 + topic_label * direction + s(abs_turn) + (1 | conv_id))

cat("Benchmark start:", format(Sys.time(), "%H:%M:%S"), "\n")

benchmark_fit <- brm(
  formula = benchmark_formula,
  data = benchmark_data,
  family = gaussian(),
  prior = my_priors,
  chains = 2,
  iter = benchmark_iter,
  warmup = benchmark_warmup,
  seed = 123,
  backend = backend_to_use,
  refresh = 200,
  control = sampling_ctrl
)

cat("Benchmark end:", format(Sys.time(), "%H:%M:%S"), "\n")

rm(benchmark_fit)
gc()
```

## 7) Fit models (publishable run)

Outputs:
- bayes_pub_draws_topic_direction.csv
- bayes_pub_hierarchical_stats.csv
- bayes_pub_turn_smooth_draws.csv
- bayes_pub_model_diagnostics.csv
- bayes_pub_models (RDS files per metric)

```{r}
draws_topic_dir <- list()
hierarchy_stats <- list()
smooth_draws <- list()
diagnostics <- list()

models_dir <- "bayes_pub_models"
dir.create(models_dir, showWarnings = FALSE, recursive = TRUE)

for (m in unique(df_long$metric)) {

  cat("\n===================================================\n")
  cat("STARTING:", m, "Time:", format(Sys.time(), "%H:%M:%S"), "\n")

  d <- df_long %>% filter(metric == m)

  f <- bf(z ~ 0 + topic_label * direction + s(abs_turn) + (1 | conv_id))

  fit <- brm(
    formula = f,
    data = d,
    family = gaussian(),
    prior = my_priors,
    chains = chains_pub,
    iter = iter_pub,
    warmup = warmup_pub,
    seed = 123,
    backend = backend_to_use,
    refresh = 200,
    control = sampling_ctrl
  )

  # Save fitted model to disk for auditability
  saveRDS(fit, file = file.path(models_dir, paste0("brm_", m, ".rds")))

  # Diagnostic summaries
  summ <- summary(fit)
  rhat_max <- max(summ$fixed[,"Rhat"], na.rm = TRUE)
  ess_min <- min(summ$fixed[,"Bulk_ESS"], na.rm = TRUE)

  diagnostics[[m]] <- tibble(
    metric = m,
    rhat_max_fixed = as.numeric(rhat_max),
    ess_min_fixed = as.numeric(ess_min)
  )

  # Hierarchical stats and ICC-like share
  vc <- VarCorr(fit)
  conv_sd <- vc$conv_id$sd[1]
  residual_sd <- if (!is.null(vc$residual$sd[1])) vc$residual$sd[1] else NA
  icc <- (conv_sd^2) / ((conv_sd^2) + (residual_sd^2))

  hierarchy_stats[[m]] <- tibble(
    metric = m,
    conv_sd = as.numeric(conv_sd),
    residual_sd = as.numeric(residual_sd),
    icc_like = as.numeric(icc)
  )

  # Topic by direction fixed effects draws
  dr <- as_draws_df(fit)
  b_cols <- names(dr)[str_detect(names(dr), "^b_")]

  b_long <- dr %>%
    select(all_of(b_cols), .chain, .iteration, .draw) %>%
    pivot_longer(cols = all_of(b_cols),
                 names_to = "param",
                 values_to = "value") %>%
    mutate(metric = m)

  b_topic_dir <- b_long %>%
    filter(str_detect(param, "^b_topic_label")) %>%
    mutate(
      topic_clean = str_remove(param, "^b_topic_label"),
      topic_clean = str_replace_all(topic_clean, ":direction", " x direction")
    )

  draws_topic_dir[[m]] <- b_topic_dir

  # Smooth term draws for turn trend plotting
  turn_grid <- tibble(
    abs_turn = sort(unique(d$abs_turn)),
    topic_label = levels(d$topic_label)[1],
    direction   = levels(d$direction)[1],
    conv_id     = NA
  )

  smooth_epred <- posterior_epred(fit, newdata = turn_grid, re_formula = NA)
  smooth_df <- as_tibble(t(smooth_epred)) %>%
    mutate(abs_turn = turn_grid$abs_turn) %>%
    pivot_longer(cols = -abs_turn, names_to = "draw_id", values_to = "epred") %>%
    mutate(metric = m)

  smooth_draws[[m]] <- smooth_df

  # Save incremental outputs
  write_csv(bind_rows(draws_topic_dir), "bayes_pub_draws_topic_direction.csv")
  write_csv(bind_rows(hierarchy_stats), "bayes_pub_hierarchical_stats.csv")
  write_csv(bind_rows(smooth_draws), "bayes_pub_turn_smooth_draws.csv")
  write_csv(bind_rows(diagnostics), "bayes_pub_model_diagnostics.csv")

  rm(fit, dr, b_long, b_topic_dir, vc, smooth_epred, smooth_df)
  gc()

  cat("DONE:", m, "\n")
}

cat("\nALL FINISHED. Outputs saved in working directory.\n")
```

## 8) Diagnostic checklist

After the run completes, inspect:
- `bayes_pub_model_diagnostics.csv` for Rhat and ESS thresholds
- `bayes_pub_hierarchical_stats.csv` for ICC-like values
- Trace plots for any metrics with Rhat above 1.01 or low ESS
